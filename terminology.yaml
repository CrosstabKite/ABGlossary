
- title: "Glossary of terms: The language, lexicon and lingo of testing"
  link: https://support.google.com/optimize/answer/6211939?hl=en&ref_topic=6315913
  orgs:
    - Google
  authors:
  terms:
    A/B test: >-
      "A randomized experiment using two or more variants of the same web page (A and
      B). Variant A is the original and variant B through n each contain at least one
      element that is modified from the original."
    A/B/n test: synonym for "A/B test"
    experiment: >-
      "a method of testing different variants of web pages to determine which is most
      effective at achieving an objective. Examples include A/B, redirect, and
      multivariate tests."
    change list: >-
      "A list of changes that are applied to one or more elements on your web page when
      your visitor arrives."
    combination: >-
      "In a multivariate test (MVT) with multiple sections, a combination is a set of
      changes created by selecting a single variant from each section. The total number
      of combinations is the product of the number of variants in each section."
    container: >-
      "holds all of the...configuration information for a website’s experiences (tests
      and personalizations). A "container snippet" is a small piece of JavaScript code
      that's added to a web page where you want to...present users with a new
      experience, while a "container ID" is an alphanumeric string (e.g. "GTM-A1B2CD")
      that uniquely identifies it. You will also see container ID referred to as
      "OPT_MEASUREMENT_ID" in code samples."
    leader: >-
      "The variant that performs best against the objective."
    modeled objective rate: >-
      "The long-run average of where Optimize expects the value of the specified
      objective to land. For example, if you have a revenue objective, Optimize is
      modeling the long-run average revenue per session. If you have a pageview
      objective, Optimize is modeling the long-run average pageviews per session."
    multivariate test: >-
      "An experiment that tests two or more sections to understand their effects on each
      other. For example, variants of a headline can be tested at the same time as
      variants of a hero image. Instead of showing which page variant is most effective
      (as in an A/B test), a multivariate test identifies the most effective combination
      of variants. Rather than the two or three page variants found in simple A/B tests,
      multivariate tests frequently test multiple variants of multiple page elements
      simultaneously."
    objective: >-
      "the website metric you wish to optimize. Think of objectives as the metrics or
      activities that you'll measure your variants against, e.g. pageviews, bounce rate,
      time on site, etc."
    original: >-
      "Your current web page, prior to any modification." Also known as the "baseline".
    probability to beat original: >-
      "(PBO) The probability that a given variant will result in a better conversion
      rate than the original."
    probability to be best: >-
      "(PBB) The probability that a given variant performs better than all other
      variants."
    redirect destination: >-
      "A separate and unique web page used as a variant in a redirect experiment."
    redirect test: >-
      "A type of A/B test that allows you to test separate web pages against each other.
      In redirect tests variants are identified by URL or path instead of an element(s)
      on the page. Redirect tests are useful when you want to test two very different
      landing pages, or a complete redesign of a page."
    section: >-
      "a group of variants, each of which can modify multiple page elements (e.g. a
      headline, image, or button). An A/B test only contains one section (with one or
      more variants), while a multivariate test (MVT) tests multiple sections
      simultaneously." Also known as "factor", "element".
    session: >-
      "The period of time a user is active on your site or app. By default, if a user is
      inactive for 30 minutes or more, any future activity is attributed to a new
      session. Users that leave your site and return within 30 minutes are counted as
      part of the original session."
    targeting rules: >-
      "An experiment will only execute when its targeting rules are met. Rules can be
      used to target everything from geographic regions to specific user behavior."
    treatment: >-
      "The original and all variants."
    variant: >-
      "A variant can be anything from a change to a single element, or changes to
      multiple elements, or a totally different page in an experiment. In an A/B test
      the unit of variant can be a web page or an element of a web page. In a
      multivariate experiment (MVT) you have multiple variants of each section." Also
      known as "variation", "level".

- title: Google Optimize Features
  link: https://marketingplatform.google.com/about/optimize/features/
  orgs:
    - Google
  authors:
  terms:
    A/B test: >-
      "let you test multiple versions of the same web page to learn which page works
      best for your users."
    A/B/n test: synonym for "A/B test"
    multivariate test: >-
      "allow you to test multiple elements on a page to see which combination achieves
      your goals."
    split URL test: >-
      "a type of A/B test that lets you test separate pages against each other. With
      redirect tests, the test variants are identified by URL instead of page element
      which is most useful when testing two very different landing pages or a complete
      page redesign."
    redirect test: synonym for "spit URL test"
    server-side experiment:
    activity log: >-
      "keeps a chronological history of changes. View changes made to an account or
      container, when they were made, and who made them."
    experiment preview: >-
      "In preview mode, you can see how the variant you’ve created looks on different
      platforms and devices before you run your experiment."
    system objective: >-
      "common objectives that are often used in experiments. You can add a system
      objective to any experiment with Optimize."

- title: Under the Hood of Uber’s Experimentation Platform
  link: https://eng.uber.com/xp/
  orgs:
    - Uber
  authors:
    - Anirban Deb
    - Suman Bhattacharya
    - Jeremy Gu
    - Tianxia Zhou
    - Eva Feng
    - Mandie Liu
  terms:
    A/B test: >-
      "randomly split users into control and treatment groups to compare the decision
      metrics between these groups and determine the experiment’s treatment effects."
    A/B/N test:
    hypothesis:
    metric:
    decision metric:
    continuous metric: >-
      "contain one numeric value column, e.g., gross bookings per user."
    proportion metric: >-
      "contain one binary indicator value column, e.g., to test the proportion of users
      who complete any trips after sign-up."
    ratio metric: >-
      "contain two numeric value columns, the numerator values and the denominator
      values, e.g., the trip completion ratio"
    treatment effect:
    lift: synonym for 'treatment effect'. Also referred to as 'causal lift'.
    control group:
    treatment group:
    multi-armed bandit:
    regression:
    universal holdout: >-
      "used to measure the long-term effects of all experiments for a specific domain."
    sample size imbalance: >-
      "the sample size ratio in the control and treatment groups is significantly
      different from what was expected."
    flicker: >-
      a user who has "switched between control and treatment groups. For example, a
      rider purchases a new Android cell phone to replace an old iPhone, while the
      treatment of the experiment was only configured for iOS."
    pre-experiment bias:
    sequential test:
    
- title: Ramblings on Experimentation Pitfalls, Part 1
  link: https://eng.lyft.com/ramblings-on-experimentation-pitfalls-dd554ff87c0e
  orgs:
    - Lyft
  authors:
    - "timothybrownsf"
  terms:
    rollout: >- 
      A protocol for a staged, test-based launch of a new idea that uses a "holdout"
      group in addition to control and treatment groups. In the first stage, for
      example, the allocation of subjects might be (10% control, 10% treatment, 80%
      holdout). The next stage might be (50% control, 50% treatment, 0% holdout). The
      goal is to avoid the analysis mistake of weighting each observational subject
      equally across all stages, rather than (correctly) doing the analysis for each
      stage separately.
    variant:
    control:
    treatment:
    split:
    holdout: >-
      a group of subjects (users) who are in the target population for an A/B
      test, but not part of either the control or treatment group.

- title: Increasing the sensitivity of A/B tests by utilizing the variance estimates of experimental units
  link: https://research.fb.com/blog/2020/10/increasing-the-sensitivity-of-a-b-tests-by-utilizing-the-variance-estimates-of-experimental-units/
  orgs:
    - Facebook
  authors:
    - Kevin Liou
    - Sean Taylor
  terms:
    online experiment:
    A/B test: 
    experimental unit:
    randomized field experiment: synonym of "A/B test"
    sensitivity: >-
      A successful A/B test "must be capable of detecting effects that product changes
      generate. From a hypothesis-testing perspective, experimenters aim to have high
      statistical power".
    metric:
    treatment effect:
    regression adjustment: synonym of "CUPED"
    CUPED: synonym of "regression adjustment"
    sample size:
    stratification:

- title: Optimization Glossary
  link: https://www.optimizely.com/optimization-glossary/
  orgs: 
    - Optimizely
  authors:
  terms:
    A/A test: >-
      "A method of comparing two versions of a webpage or mobile app experience against
      each other in order to test the accuracy of the testing tool."
    A/B/N test: >-
      "A method of comparing multiple versions of webpage or mobile app against each
      other to determine which performs best."
    A/B test: >-
      "A method of comparing two versions of a webpage or mobile app experience
      against each other to determine which performs best."
    bucket test: >-
      "A method of testing two versions of a website or app against one another to see
      which one performs better on specified key metrics. Synonym for split testing."
    canary test: >-
      "A way to reduce risk and validate new software by releasing software to a small
      percentage of users."
    multi-armed bandit: >-
      "A form of A/B testing that uses machine learning algorithms to dynamically
      allocate traffic to variations that are performing well."
    multivariate test: >-
      "A technique for testing a hypothesis where multiple variables are modified, in
      order to determine the best combination of variations on those elements of a
      website or mobile app."
    split test: >-
      "A strategy for conducting controlled, randomized experiments with the goal of
      improving a conversion metric on a website or mobile app."

- title: Switchback Tests and Randomized Experimentation Under Network Effects at
    DoorDash
  link: https://medium.com/@DoorDash/switchback-tests-and-randomized-experimentation-under-network-effects-at-doordash-f1d938ab7c2a
  orgs: 
    - DoorDash
  authors:
    - David Kastelman
    - Raghav Ramesh
  terms:
    network effect: >-
      adding a subject to the treatment group affects the experience of subjects in the
      control group, so the two groups are not independent
    switchback test: >-
      a treatment protocol where the treatment window is divided into time periods. In
      each time period the entire sample receives one variant (e.g. treatment or
      control), chosen randomly.
    A/A test: >-
      "a dummy experiment where there’s no actual difference between treatment and
      control".
    bias: >-
      occurs when randomization is compromised so that subjects in the treatment and
      control groups are not the same, on average
    margin of error: >-
      "how much uncertainty exists in our estimate of the impact of an intervention."

- title: Server-Side Testing
  link: https://vwo.com/server-side-testing/
  orgs:
    - VWO
  authors:
  terms:
    server-side test: >-
      "a method of A/B testing wherein the variations of a test are rendered directly
      from the webserver"
    client-side test: >-
      website variations are delivered by the visitors' browser (client). "Limited to
      cosmetic changes; experiments revolve around the design, placement, messaging of
      the key elements on a web page"
    feature flag: >-
      "enable you to turn your features ON/OFF and thereby alter user experience,
      without having to deploy new code."
    feature toggle: synonym for 'feature flag'
    staged rollout: >-
      make a particular feature available to only a small percentage of the audience,
      systematically making it available to all customers in progressive stages.
    flicker effect: >-
      "the phenomenon wherein the original content of a page appears for a very short
      while before the variation loads." AKA the "Flash Of Original Content".

- title: A/B Testing Guide
  link: https://vwo.com/ab-testing/
  orgs:
    - VWO
  authors:
  terms:
    A/B test: >-
      the "process of showing two variants of the same web page to different segments of
      website visitors at the same time and comparing which variant drives more
      conversions."
    variant:
    variation: a version of your system "with changes that you want to test"
    metric:
    hypothesis:
    split URL test: >-
      "testing multiple versions of your webpage hosted on different URLs." Differs from
      A/B testing in that the latter implements variations at the same URL. "preferred
      when significant design changes are necessary and don’t want to touch the existing
      website design."
    multivariate testing: >-
      "changes are made to multiple sections of a webpage, and variations are created
      for all the possible combinations...you can test all the combinations within a
      single test."
    multipage testing: >-
      "Multipage testing is a form of experimentation where you can test changes to
      particular elements across multiple pages"

- title: 'It’s All A/Bout Testing: The Netflix Experimentation Platform'
  link: https://netflixtechblog.com/its-all-a-bout-testing-the-netflix-experimentation-platform-4e1ca458c15#annotations:pRwJrnugEeucIWdt9l-iJw
  orgs:
    - Netflix
  authors:
    - Steve Urban
    - Rangarajan Sreenivasan
    - Vineet Kannan
  terms:
    A/B test: an experiment with a control group and one or more experimental groups
    control group:
    experimental group:
    cell: a group of users
    treatment:
    default cell: >-
      the group of users that receives the control treatment, which is the same
      experience as all Netflix users not in the test.
    metric: outcome of importance, typically streaming hours and retention
    allocation: how users are assigned to a test
    batch allocation:
    real-time allocation:
    stratified sampling:

- title: 
    Improving Experimentation Efficiency at Netflix with Meta Analysis and Optimal
    Stopping
  link: https://netflixtechblog.com/improving-experimentation-efficiency-at-netflix-with-meta-analysis-and-optimal-stopping-d8ec290ae5be
  orgs:
    - Netflix
  authors:
    - Gang Su
    - Ian Yohai
  terms:
    power analysis:
    seasonality:
    novelty effect:
    treatment effect:
    HARKing:
    p-hacking:
    fixed effect model:
    random effect model:
    heterogeneous treatment effect:
    sequential triangular testing:
    group sequential testing:
    

- title: Statistics For Experimenters, 2nd edition
  link: https://statisticsforexperimenters.net/
  orgs:
  authors: 
    - George E.P. Box
    - J. Stuart Hunter
    - William G. Hunter
  terms:
    factor: controllable input variable
    level: >-
      a value of a factor. If the factor were font color, the levels might be blue and
      black.
    response: output variable
    error: >-
      "variability not explained by known influences." Also, "the fluctuation that
      occurs from one repetition to another". AKA experimental error, experimental
      variation.
    run: >-
      "an experimental run has been performed when an apparatus has been set up and
      allowed to function under a specific set of experimental conditions."
    block: >-
      a portion of experimental material expected to be more homogeneous than the
      aggregate. 
    blocking: >-
      comparisons within blocks have greater precision because between-block variability
      is eliminated. For example, testing shoe materials by asking each person to wear
      the new shoe on either their right or left foot, randomly.
    randomization: >-
      force unknown discrepancies between treatments to contribute homogeneously to the
      errors of each. This should be done after 'known' sources of discrepancy are
      removed by holding them constant or blocking.
    factorial design: >-
      conduct experiment runs in all possible combinations of levels across multiple
      factors.
  
- title: Experimentation at Tubi
  link: https://code.tubitv.com/experimentation-at-tubi-82f35afe2732
  orgs: 
    - Tubi
  authors:
    - Change She
  terms:
    experiment engine:
    namespace: represents "a group of mutually exclusive experiments"
    assignment: mapping experiment targets to segments, done by a namespace
    targets: >- 
      "e.g. devices, users, IPs, etc"
    segment: >-
      "a group of experimental targets, allocated to at most one experiment"
    allocation: mapping of segments to experiments
    treatment group:
    phase: >-
      stage of an experiment. Different phases "reserve differing amounts of segments".
    condition: logic that refines segments, e.g. "only new users".
    start date:
    end date:
    override: >-
      "inclusion of development and test devices in particular experiments and treatment
      groups"
    graduate: progression of an experiment to the next phase
    exposure: >-
      the event where a target is impacted by a namespace, experiment, and treatment
      group.
    metric: 
    Northstar metrics: >-
      "A small subset of metrics that are closest to top-level company KPIs"
    do-no-harm criterion: >-
      "an experiment that harms any Northstar metric cannot be graduated
      without...a...review process."
    uneven split: a large difference between the number of users in each group
    
- title: Trustworthy Online Controlled Experimentation
  link: https://experimentguide.com/
  orgs:
    - Microsoft
    - Amazon
    - Google
    - LinkedIn
  authors: 
    - Ron Kohavi
    - Diane Tang
    - Ya Xu
  terms:
    online controlled experiment: >-
      "users are split between variants in a persistent manner"
    instrumentation: monitoring and logging of a system
    metric:
    overall evaluation criterion (OEC): >-
      "A quantitative measure of the experiment's objective...For example...active days
      per user..." May be a combination of metrics. AKA the response or the dependent
      variable.
    parameter: >-
      "A controllable experimental variable that is thought to influence...metrics of
      interest."
    value: >-
      a specific value assigned to a variable parameter. If the parameter is font color,
      for example, then a value might be 'blue'.
    variant: >-
      the experience being tested, created by assigning values to all the parameters. In
      an A/B test, A and B represent the two variants. Similarly, 'control' and
      'treatment' are variants.
    control: >-
      a special variant, namely the "existing version on which to run the comparison."
    treatment:
    unit: >-
      "e.g., users or pages"
    randomization (unit): application of a pseudo-random process to map units to
      variants. For online audiences, it is common to use users as the randomization
      unit, but it is also possible to randomize by pages, sessions, or user-day.
    
  